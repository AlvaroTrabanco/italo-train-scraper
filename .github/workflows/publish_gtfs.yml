name: Build & Publish Italo GTFS + Stop Report

on:
  workflow_dispatch:
  schedule:
    - cron: "12 * * * *" # hourly (UTC)

permissions:
  contents: write

concurrency:
  group: italo-gtfs-publish
  cancel-in-progress: true

jobs:
  build_publish:
    runs-on: ubuntu-latest
    env:
      SKIP_RUN: "0"
    steps:
      # Main branch checkout (code)
      - name: Checkout (main)
        uses: actions/checkout@v4

      # gh-pages checkout (persisted state + published site)
      - name: Checkout (gh-pages)
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Compute vars
        id: vars
        run: |
          # Service date is always "tomorrow"
          echo "SERVICE_DATE=$(date -u -d 'tomorrow' +%Y%m%d)" >> $GITHUB_OUTPUT
          # Collection window = 8 days
          echo "WINDOW_HOURS=192" >> $GITHUB_OUTPUT

      - name: Restore persisted state (inventory + normalized_latest)
        run: |
          mkdir -p tmp_prev normalized_latest

          # Restore inventory if present
          if [ -f gh-pages/reports/stop_inventory.json ]; then
            cp gh-pages/reports/stop_inventory.json tmp_prev/stop_inventory.json
            echo "Restored stop_inventory.json"
          else
            echo "No previous stop_inventory.json found"
          fi

          # Restore cumulative normalized_latest if present
          if [ -d gh-pages/state/normalized_latest ]; then
            cp -R gh-pages/state/normalized_latest/. normalized_latest/
            echo "Restored normalized_latest ($(ls -1 normalized_latest | wc -l) files)"
          else
            echo "No previous normalized_latest found"
          fi

      - name: Check 8-day window (auto-stop)
        run: |
          mkdir -p public/state

          NOW_EPOCH=$(date -u +%s)

          if [ -f gh-pages/state/collection_started_epoch.txt ]; then
            START_EPOCH=$(cat gh-pages/state/collection_started_epoch.txt | tr -d '\n\r')
            echo "Collection started epoch: $START_EPOCH"
          else
            START_EPOCH=$NOW_EPOCH
            echo "Collection started epoch (new): $START_EPOCH"
          fi

          # Persist the start epoch every run
          echo "$START_EPOCH" > public/state/collection_started_epoch.txt

          ELAPSED=$((NOW_EPOCH - START_EPOCH))
          LIMIT=$((8 * 24 * 3600))

          if [ "$ELAPSED" -ge "$LIMIT" ]; then
            echo "8-day collection window reached. Skipping scrape/normalize/build."
            STOPPED_UTC=$(date -u +%Y%m%dT%H%M%SZ)
            echo "{ \"status\": \"stopped\", \"collection_started_epoch\": $START_EPOCH, \"stopped_utc\": \"$STOPPED_UTC\" }" > public/state/collection_status.json
            echo "SKIP_RUN=1" >> $GITHUB_ENV
          else
            NOW_UTC=$(date -u +%Y%m%dT%H%M%SZ)
            echo "{ \"status\": \"running\", \"collection_started_epoch\": $START_EPOCH, \"now_utc\": \"$NOW_UTC\", \"elapsed_seconds\": $ELAPSED, \"limit_seconds\": $LIMIT }" > public/state/collection_status.json
            echo "SKIP_RUN=0" >> $GITHUB_ENV
          fi

      - name: Scrape (from trains.txt)
        if: env.SKIP_RUN != '1'
        run: |
          mkdir -p out

          python scraper/italo_scrape.py \
            --trains-file scraper/trains.txt \
            --outdir out \
            --slice-size 999999 \
            --slice-index 0 \
            --skip-empty

          RUN_UTC=$(find out -maxdepth 1 -type d -name "20*T*Z" -printf "%f\n" | sort | tail -n 1)
          if [ -z "$RUN_UTC" ]; then
            echo "Could not detect RUN_UTC folder under out/"
            ls -la out || true
            exit 1
          fi

          echo "RUN_UTC=$RUN_UTC" >> $GITHUB_ENV
          echo "Scrape run folder: out/$RUN_UTC"

      - name: Normalize + merge into cumulative normalized_latest
        if: env.SKIP_RUN != '1'
        run: |
          OUTDIR="out/$RUN_UTC"
          NORMDIR="normalized/$RUN_UTC"
          mkdir -p "$NORMDIR"
          mkdir -p normalized_latest

          python scraper/normalize_italo.py --input-dir "$OUTDIR" --output-dir "$NORMDIR"

          # Merge this run into cumulative normalized_latest (overwrite per train)
          cp -f "$NORMDIR"/*.normalized.json normalized_latest/ 2>/dev/null || true

          echo "normalized_latest now has $(ls -1 normalized_latest | wc -l) files"

      - name: Build GTFS (stable + dated) from cumulative normalized_latest
        if: env.SKIP_RUN != '1'
        env:
          SERVICE_DATE: ${{ steps.vars.outputs.SERVICE_DATE }}
        run: |
          SERVICE_DATE="${{ steps.vars.outputs.SERVICE_DATE }}"
          WINDOW_HOURS="${{ steps.vars.outputs.WINDOW_HOURS }}"
          NORMDIR="normalized_latest"

          mkdir -p gtfs public/gtfs public/reports
          mkdir -p public/state/normalized_latest
          touch public/.nojekyll

          python scraper/build_gtfs.py \
            --normalized-dir "$NORMDIR" \
            --service-date "$SERVICE_DATE" \
            --out-zip "gtfs/italo_$SERVICE_DATE.zip"

          # Stable + dated copies to Pages
          cp "gtfs/italo_$SERVICE_DATE.zip" "public/gtfs/italo_latest.zip"
          cp "gtfs/italo_$SERVICE_DATE.zip" "public/gtfs/italo_$SERVICE_DATE.zip"

          # Persist cumulative normalized_latest to Pages state
          cp -f normalized_latest/*.normalized.json public/state/normalized_latest/ 2>/dev/null || true

          NORM_COUNT=$(ls -1 normalized_latest | wc -l)
          TRIPS_COUNT=$(python3 -c 'import os,zipfile; sd=os.environ["SERVICE_DATE"]; zp=f"gtfs/italo_{sd}.zip"; z=zipfile.ZipFile(zp); lines=z.read("trips.txt").decode("utf-8","replace").splitlines(); print(max(0,len(lines)-1))' )

          cat > public/gtfs/latest.json <<EOF
          {
            "run_utc": "$RUN_UTC",
            "service_date": "$SERVICE_DATE",
            "window_hours": $WINDOW_HOURS,
            "latest_zip": "italo_latest.zip",
            "dated_zip": "italo_$SERVICE_DATE.zip",
            "normalized_latest_files": $NORM_COUNT,
            "gtfs_trips": $TRIPS_COUNT
          }
          EOF

          if [ -f public/index.html ]; then
            echo "index.html already in public/"
          elif [ -f index.html ]; then
            cp index.html public/index.html
          else
            echo "No index.html found (expected public/index.html in repo)."
          fi

      - name: Stop coordinates report (cumulative)
        if: env.SKIP_RUN != '1'
        run: |
          INV_IN="tmp_prev/stop_inventory.json"
          python scraper/report_stops.py \
            --normalized-dir "normalized/$RUN_UTC" \
            --coordinates "coordinates.csv" \
            --out-dir "public/reports" \
            --run-utc "$RUN_UTC" \
            $( [ -f "$INV_IN" ] && echo "--inventory-in $INV_IN" ) \
            --inventory-out "public/reports/stop_inventory.json" \
            --window-hours "${{ steps.vars.outputs.WINDOW_HOURS }}"

      - name: Publish to GitHub Pages (gh-pages)
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          publish_branch: gh-pages
          keep_files: true