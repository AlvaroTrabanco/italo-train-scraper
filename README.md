Here is a clean, minimal, accurate README for your current setup (cumulative GTFS, 8-day window, local rebuild script, manual coordinates).

You can paste this into README.md.

‚∏ª

Italo Train Scraper ‚Üí GTFS Builder

This repository:
	1.	Scrapes Italo train schedules from
https://italoinviaggio.italotreno.com/api/RicercaTrenoService
	2.	Normalizes them into a stable intermediate format
	3.	Builds a cumulative GTFS feed
	4.	Publishes:
	‚Ä¢	A stable GTFS URL
	‚Ä¢	Stop coordinate reports
	‚Ä¢	Collection status metadata

‚∏ª

üåç Public URLs

Base:

https://alvarotrabanco.github.io/italo-train-scraper/

Stable GTFS:

/gtfs/italo_latest.zip

Latest dated GTFS:

/gtfs/italo_YYYYMMDD.zip

Stop coordinate report:

/reports/stops_report_latest.csv

Collection status:

/state/collection_status.json


‚∏ª

üß† System Logic

GitHub Actions (Hourly)

Workflow: .github/workflows/publish_gtfs.yml

Every hour:
	1.	Checks 8-day collection window.
	2.	Restores cumulative normalized state from gh-pages/state/normalized_latest.
	3.	Scrapes trains from scraper/trains.txt.
	4.	Normalizes new data.
	5.	Merges into cumulative normalized_latest.
	6.	Builds GTFS from cumulative dataset.
	7.	Generates stop coordinate report.
	8.	Publishes everything to gh-pages.

The GTFS grows more complete over the 8-day window.

After 8 days, the workflow stops scraping but keeps publishing status.

‚∏ª

üìÇ Important Folders

Folder	Purpose
scraper/	All Python scripts
normalized/	Per-run normalized output
normalized_latest/	Cumulative normalized set (local only)
coordinates.csv	Manual stop coordinates
public/	Published to GitHub Pages
state/	Persisted cumulative state from gh-pages


‚∏ª

üß© Scripts Overview

scraper/italo_scrape.py

Scrapes raw JSON per train number.

scraper/normalize_italo.py

Extracts scheduled stops and times.

scraper/build_gtfs.py

Builds GTFS zip from normalized JSON + coordinates.csv.

scraper/report_stops.py

Generates:
	‚Ä¢	stops_report_latest.csv
	‚Ä¢	stops_report_latest.md
	‚Ä¢	stop_inventory.json

scraper/make_normalized_latest.py

Merges all local normalized runs into normalized_latest/.

scraper/rebuild_reports.sh

One-command local rebuild of reports from GitHub cumulative state.

‚∏ª

üõ† Local Workflow (Manual Coordinates Loop)

1Ô∏è‚É£ Rebuild local reports from cumulative state

From repo root:

./scraper/rebuild_reports.sh

This:
	‚Ä¢	fetches gh-pages
	‚Ä¢	restores state/normalized_latest
	‚Ä¢	rebuilds normalized_latest
	‚Ä¢	regenerates reports/stops_report_latest.csv

‚∏ª

2Ô∏è‚É£ Check missing coordinates

Open:

reports/stops_report_latest.csv

Filter by:
	‚Ä¢	MISSING_COORDINATES
	‚Ä¢	NEW_NOT_IN_COORDINATES

These are the stops that need lat/lon.

‚∏ª

3Ô∏è‚É£ Edit coordinates

File:

coordinates.csv

Format:

location_name,lat,lon

Example:

Agropoli Castellabate,40.351234,14.998765

Names must match exactly the stop names in normalized JSON.

‚∏ª

4Ô∏è‚É£ Commit and push

git add coordinates.csv
git commit -m "Update Italo stop coordinates"
git push

Next GitHub Actions run will:
	‚Ä¢	rebuild GTFS
	‚Ä¢	update stop report
	‚Ä¢	publish updated feed

‚∏ª

üîÅ Optional: Build GTFS locally

python3 scraper/make_normalized_latest.py

python3 scraper/build_gtfs.py \
  --normalized-dir normalized_latest \
  --service-date $(date -u -d 'tomorrow' +%Y%m%d) \
  --out-zip gtfs/local_test.zip


‚∏ª

üìä How to Verify Cumulative Collection

Open:

/gtfs/latest.json

Key fields:
	‚Ä¢	normalized_latest_files
	‚Ä¢	gtfs_trips

These should increase (or remain stable) over time.

‚∏ª

‚ö†Ô∏è Important Notes
	‚Ä¢	Coordinates are matched by stop_name (exact string match).
	‚Ä¢	If a stop appears in GTFS but not in report, regenerate local report from cumulative state.
	‚Ä¢	--skip-empty in scraper means only active trains are captured at runtime.

‚∏ª

üßπ 8-Day Collection Window

The workflow automatically stops scraping after 8 days.

Status is visible at:

/state/collection_status.json


‚∏ª

If you‚Äôd like, I can also generate:
	‚Ä¢	A diagram of the architecture
	‚Ä¢	A short ‚Äúdeveloper notes‚Äù section
	‚Ä¢	Or a trimmed README version suitable for public visibility